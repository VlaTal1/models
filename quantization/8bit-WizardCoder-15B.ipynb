{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> WizardCoder-15B-V1.0\n",
    "# <center> 8 bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Donwnloads (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install  -Uq transformers\n",
    "!pip3 install  -Uq torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip3 install  -Uq sentencepiece\n",
    "!pip3 install  -Uq accelerate\n",
    "!pip3 install  -Uq bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading model to VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_skip_modules = [\n",
    "        \"lm_head\", \n",
    "        \"transformer.wte\", \n",
    "        \"transformer.wpe\", \n",
    "        \"transformer.drop\", \n",
    "        \"transformer.h.0\", \n",
    "        \"transformer.h.1\", \n",
    "        \"transformer.h.39\", \n",
    "        \"transformer.h.38\", \n",
    "        \"transformer.h.37\", \n",
    "        \"transformer.h.36\", \n",
    "        \"transformer.h.35\"\n",
    "        ]\n",
    ")\n",
    "\n",
    "BASE_MODEL_ID = 'WizardLM/WizardCoder-15B-V1.0'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID,\n",
    "                                            device_map=\"auto\", # {\"\":0} - load all wieghts to vram\n",
    "                                            trust_remote_code=True,\n",
    "                                            quantization_config=quantization_config,\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is your prompt\n",
    "prompt = '''Write Java class Teacher using Loombok annotation with fields id, name, surname, department.  Write entity for DB TeacherDTO class based on Teacher class. Use JPA jakarta annotations only for TeacherDTO classes to save in data base'''\n",
    "\n",
    "# Here is template that model can understand your prompt and make response \n",
    "prompt_template = f'''Below is an instruction that describes a task. Write a response that appropriately completes the request\n",
    "\n",
    "### Instruction: {prompt}\n",
    "\n",
    "### Response:'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    top_p=0.0,\n",
    "    top_k=1,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=600,\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    ")   \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoding = tokenizer(prompt_template, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode(): \n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "        do_sample=False,\n",
    "        use_cache = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
